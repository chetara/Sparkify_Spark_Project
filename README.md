# ğŸ§ Sparkify ETL Project (PySpark)

This project builds a **data pipeline using Apache Spark** to process song and log data for Sparkify, a fictional music streaming service. The result is a set of **analytically useful tables** written in Parquet format, structured in a star schema.

---

## ğŸ“ Project Structure

```bash
Sparkify_spark_project/
â”œâ”€â”€ data/                  # Raw input data (songs & logs)
â”‚   â”œâ”€â”€ song_data/
â”‚   â””â”€â”€ log_data/
â”œâ”€â”€ etl/                   # ETL pipeline scripts
â”‚   â”œâ”€â”€ test.py
â”‚   â”œâ”€â”€ load_data.py
â”‚   â”œâ”€â”€ transform_songs.py
â”‚   â”œâ”€â”€ transform_users.py
â”‚   â”œâ”€â”€ build_songplays.py
â”œâ”€â”€ outputs/               # Output folder for Parquet tables
â”œâ”€â”€ .venv/                 # Python virtual environment (excluded from git)
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # This file

``` 
# âš™ï¸ Spark Architecture Behind This ETL Project

This ETL pipeline is powered by **Apache Spark**, a distributed data processing engine. Even though this project runs locally, it uses the **same execution model** that powers Spark on clusters like AWS EMR, Databricks, and Hadoop YARN.

---

## ğŸ§  What Is Happening Under the Hood?

### âœ… Components Used by Spark:

| Component       | Role                                                                 |
|----------------|----------------------------------------------------------------------|
| **Driver**      | Coordinates the job, builds the DAG, sends tasks to executors        |
| **Executors**   | Perform the actual data processing (reading, filtering, writing)     |
| **Cluster Manager** | Manages resources. In this project, it's `local[*]` (your own machine) |

```python
spark = SparkSession.builder.appName("Sparkify ETL").getOrCreate()
```

## ğŸš€ What It Does
âœ… Reads raw JSON files of song and log data

âœ… Cleans and transforms the data using PySpark

âœ… Creates a star schema of tables:

songs (dimension)

artists (dimension)

users (dimension)

time (dimension)

songplays (fact table)

âœ… Writes each table in Parquet format for efficient querying

## ğŸ› ï¸ Technologies Used

Tool	Purpose
Python 3.10+	Core programming language
PySpark 3.x	Distributed data processing & transformation
Hadoop 3.3.1	Required for file I/O on Windows (via winutils.exe)
Parquet	Columnar output format for performance
Local filesystem	Storage backend (easily extendable to AWS)

## âš ï¸ Windows Setup Fix: Native Hadoop Error
If you get this error when reading many files:


``` bash

java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0
âœ… Solution

```

# âœ… Step-by-Step Fix:
Download winutils.exe

Download: winutils.exe (Hadoop 3.3.1)

Create Hadoop Directory


``` bash

C:\hadoop\bin\

``` 

Place the downloaded file


``` bash

C:\hadoop\bin\winutils.exe

```
Set Environment Variables in Python


``` python

import os
os.environ["HADOOP_HOME"] = "C:/hadoop"
os.environ["SPARK_SUBMIT_OPTS"] = "-Dhadoop.home.dir=C:/hadoop"

```
Bypass native file globbing using Python


``` python

import glob
file_paths = glob.glob("data/song_data/*/*/*/*.json")
file_paths = [f.replace("\\", "/") for f in file_paths]
df = spark.read.json(file_paths)

```
## ğŸ§ª ETL Process Breakdown
This project follows a classic ETL pipeline using Apache Spark. Here's a detailed breakdown of each phase:

# Step 1: Load Raw Data
â¤ What
Load raw song and log data (JSON format) into Spark DataFrames.

â¤ Why
To perform distributed processing and transformation at scale, Spark needs structured DataFrames.

â¤ How
``` python
song_df = spark.read.json(song_files)
log_df = spark.read.json(log_files).filter(col("page") == "NextSong")
Used Pythonâ€™s glob module to read file paths (helps on Windows)
```

Filtered log_df by page == "NextSong" to only include song play events

Saved raw datasets to Parquet (outputs/raw_logs/, outputs/raw_songs/)

# Step 2: Transform Song & Artist Tables
â¤ What
Extract clean, structured dimension tables: songs and artists.

â¤ Why
To enable analysis by song, artist, and metadata like duration, year, etc.

â¤ How
``` python
songs_table = song_df.select("song_id", "title", "artist_id", "year", "duration").dropDuplicates()
artists_table = song_df.select("artist_id", "artist_name", "artist_location", ...).dropDuplicates()
``` 
Ensured uniqueness with .dropDuplicates()

Wrote to outputs/songs/ and outputs/artists/

# Step 3: Transform Users & Time Tables
â¤ What
Create users and time tables from the log dataset.

â¤ Why
Useful for analyzing user behavior and session time trends.

â¤ How
``` python

users_table = log_df.select("userId", "firstName", "lastName", ...).dropDuplicates()

get_timestamp = udf(lambda ts: datetime.fromtimestamp(ts / 1000))
log_df = log_df.withColumn("start_time", get_timestamp("ts"))
time_table = log_df.select("start_time").withColumn("hour", hour("start_time"))...
```
Used Sparkâ€™s udf to convert timestamps

Extracted time components: hour, day, week, etc.

Saved to outputs/users/ and outputs/time/

# Step 4: Build Songplays Fact Table
â¤ What
Create the central fact table songplays by joining logs with songs and artists.

â¤ Why
To enable deep analysis like:

Top songs/artists by plays

User behavior over time

Song popularity by location/session

â¤ How
``` python

songplays_df = log_df.join(songs_df, ...).join(artists_df, ...)

songplays_df = songplays_df.withColumn("songplay_id", monotonically_increasing_id())
Performed joins on song == title and length == duration
```

Generated songplay_id with monotonically_increasing_id()

Wrote to outputs/songplays/
## ğŸ™Œ Author
Built by @chetara
Inspired by the Udacity Data Engineer Nanodegree Sparkify project.